library(BMR)
#
alpha    <- 0.33
vartheta <- 6
beta     <- 0.99
theta    <- 0.6667
eta    <- 1
phi    <- 1
phi_pi <- 0.1
phi_y  <- 0
rho_a  <- 0.90
rho_v  <- 0.5
BigTheta <- (1-alpha)/(1-alpha+alpha*vartheta)
kappa    <- (((1-theta)*(1-beta*theta))/(theta))*BigTheta*((1/eta)+((phi+alpha)/(1-alpha)))
psi      <- (eta*(1+phi))/(1-alpha+eta*(phi + alpha))
sigma_T <- 1
sigma_M <- 0.25
#
A <- matrix(0,nrow=0,ncol=0)
B <- matrix(0,nrow=0,ncol=0)
C <- matrix(0,nrow=0,ncol=0)
D <- matrix(0,nrow=0,ncol=0)
#Order:            yg,       y,        pi,        rn,        i,           n
F <- rbind(c(      -1,       0, -0.25*eta,         0,        0,           0),
c(       0,       0,-0.25*beta,         0,        0,           0),
c(       0,       0,         0,         0,        0,           0),
c(       0,       0,         0,         0,        0,           0),
c(       0,       0,         0,         0,        0,           0),
c(       0,       0,         0,         0,        0,           0))
G33 <- -0.25*phi_pi
G <- rbind(c(       1,       0,         0,    -1*eta, 0.25*eta,           0),
c(  -kappa,       0,      0.25,         0,        0,           0),
c(  -phi_y,       0,       G33,         0,     0.25,           0),
c(       0,       0,         0,         1,        0,           0),
c(       0,       1,         0,         0,        0,  -(1-alpha)),
c(       1,      -1,         0,         0,        0,           0))
H <- rbind(c(       0,       0,         0,         0,        0,           0),
c(       0,       0,         0,         0,        0,           0),
c(       0,       0,         0,         0,        0,           0),
c(       0,       0,         0,         0,        0,           0),
c(       0,       0,         0,         0,        0,           0),
c(       0,       0,         0,         0,        0,           0))
J <- matrix(0,nrow=0,ncol=0)
K <- matrix(0,nrow=0,ncol=0)
L <- matrix(0,nrow=6,ncol=2)
M41 <- -(1/eta)*psi*(rho_a - 1)
M <- rbind(c(   0,  0),
c(   0,  0),
c(   0, -1),
c( M41,  0),
c(  -1,  0),
c( psi,  0))
#
N <- rbind(c( rho_a,     0),
c(     0, rho_v))
Sigma <- rbind(c(sigma_T^2,         0),
c(        0, sigma_M^2))
#
dsge_obj <- new(uhlig)
dsge_obj$build(A,B,C,D,F,G,H,J,K,L,M,N)
dsge_obj$solve()
dsge_obj$P_sol
dsge_obj$Q_sol
dsge_obj$R_sol
dsge_obj$S_sol
#
dsge_obj$shocks_cov = Sigma
dsge_obj$state_space()
dsge_obj$F_state
dsge_obj$G_state
#
var_names <- c("Output Gap","Output","Inflation","Natural Int",
"Nominal Int","Labour Supply","Technology","MonetaryPolicy")
dsge_irf <- IRF(dsge_obj,12,var_names=var_names)
dsge_sim <- dsge_obj$simulate(800,200)
#
#END
library(devtools)
install_github("kthohr/BMR")
#
# Solve the NK model with Uhlig's method
#
rm(list=ls())
library(BMR)
#
alpha    <- 0.33
vartheta <- 6
beta     <- 0.99
theta    <- 0.6667
eta    <- 1
phi    <- 1
phi_pi <- 0.1
phi_y  <- 0
rho_a  <- 0.90
rho_v  <- 0.5
BigTheta <- (1-alpha)/(1-alpha+alpha*vartheta)
kappa    <- (((1-theta)*(1-beta*theta))/(theta))*BigTheta*((1/eta)+((phi+alpha)/(1-alpha)))
psi      <- (eta*(1+phi))/(1-alpha+eta*(phi + alpha))
sigma_T <- 1
sigma_M <- 0.25
#
A <- matrix(0,nrow=0,ncol=0)
B <- matrix(0,nrow=0,ncol=0)
C <- matrix(0,nrow=0,ncol=0)
D <- matrix(0,nrow=0,ncol=0)
#Order:            yg,       y,        pi,        rn,        i,           n
F <- rbind(c(      -1,       0, -0.25*eta,         0,        0,           0),
c(       0,       0,-0.25*beta,         0,        0,           0),
c(       0,       0,         0,         0,        0,           0),
c(       0,       0,         0,         0,        0,           0),
c(       0,       0,         0,         0,        0,           0),
c(       0,       0,         0,         0,        0,           0))
G33 <- -0.25*phi_pi
G <- rbind(c(       1,       0,         0,    -1*eta, 0.25*eta,           0),
c(  -kappa,       0,      0.25,         0,        0,           0),
c(  -phi_y,       0,       G33,         0,     0.25,           0),
c(       0,       0,         0,         1,        0,           0),
c(       0,       1,         0,         0,        0,  -(1-alpha)),
c(       1,      -1,         0,         0,        0,           0))
H <- rbind(c(       0,       0,         0,         0,        0,           0),
c(       0,       0,         0,         0,        0,           0),
c(       0,       0,         0,         0,        0,           0),
c(       0,       0,         0,         0,        0,           0),
c(       0,       0,         0,         0,        0,           0),
c(       0,       0,         0,         0,        0,           0))
J <- matrix(0,nrow=0,ncol=0)
K <- matrix(0,nrow=0,ncol=0)
L <- matrix(0,nrow=6,ncol=2)
M41 <- -(1/eta)*psi*(rho_a - 1)
M <- rbind(c(   0,  0),
c(   0,  0),
c(   0, -1),
c( M41,  0),
c(  -1,  0),
c( psi,  0))
#
N <- rbind(c( rho_a,     0),
c(     0, rho_v))
Sigma <- rbind(c(sigma_T^2,         0),
c(        0, sigma_M^2))
#
dsge_obj <- new(uhlig)
dsge_obj$build(A,B,C,D,F,G,H,J,K,L,M,N)
dsge_obj$solve()
dsge_obj$P_sol
dsge_obj$Q_sol
dsge_obj$R_sol
dsge_obj$S_sol
#
dsge_obj$shocks_cov = Sigma
dsge_obj$state_space()
dsge_obj$F_state
dsge_obj$G_state
#
var_names <- c("Output Gap","Output","Inflation","Natural Int",
"Nominal Int","Labour Supply","Technology","MonetaryPolicy")
dsge_irf <- IRF(dsge_obj,12,var_names=var_names)
clc
.Machine
graphics.off(); rm(list=ls());
una_funcion <- function(x) return(x^2+2) # Definimos la funcion f(x)=x^2+2
# Vamos a encontrar el minimo de esta funcion
# Primero, debemos dar al algoritmo numerico un valor inicial
x0 <- 10
# Luego usamos optim() para minimizar una_funcion
resultados <- optim(x0,una_funcion)
resultados     # El output de la funcion es una lista
resultados$par # El minimizador de la funcion se guarda en el campo "par"
otra_funcion <- function(x) return( x[1]^2+x[2]^2 )
x0 = c(10,10) # El valor inicial debe ser un vector, esta vez
optim(x0,otra_funcion) # Note que el resultado no es exactamente cero
_optim
?optim
optim(x0,otra_funcion) # Note que el resultado no es exactamente cero
optim(x0,otra_funcion,control = list(reltol=1e-12))
optim(x0,otra_funcion,control = list(reltol=1e-13))
optim(x0,otra_funcion,control = list(reltol=1e-12))
optim(x0,otra_funcion) # Note que el resultado no es exactamente cero
optim(x0,otra_funcion,method="L-BFGS-B",lower=c(-Inf,2))
# Sin embargo, debemos ingresarlos como un vector
otra_funcion <- function(x) return( x[1]^2+x[2]^2 )
x0 = c(10,10) # El valor inicial debe ser un vector, esta vez
optim(x0,otra_funcion) # Note que el resultado no es exactamente cero
# Aca, minimizamos la misma funcion sujeta a y>=2
optim(x0,otra_funcion,method="L-BFGS-B",lower=c(-Inf,2))
optim(x0,otra_funcion,lower=c(-Inf,2))
optim(x0,otra_funcion,method="L-BFGS-B",lower=c(-Inf,1))
sisa=optim(x0,otra_funcion,method="L-BFGS-B",lower=c(-Inf,1))
sisa$par
sisa$par[2]
options(digits=2)
# Aca, minimizamos la misma funcion sujeta a y>=2
optim(x0,otra_funcion,method="L-BFGS-B",lower=c(-Inf,1))
library("nloptr")
otra_funcion <- function(x) return( (x[1]-1)^2+(x[2]-2)^2 )
x0 = c(10,10)
# El algoritmo BFGS es adecuado para funciones "suaves"
resultados <- lbfgs(x0,otra_funcion)
# El algoritmo Nelder-Mead, basado en simplex, no requiere calcular derivadas
resultados <- neldermead(x0,otra_funcion)
eval_f <- function(x) {
return(
list( "objective" = x[1]*x[4]*(x[1] + x[2] + x[3]) + x[3],
"gradient"  = c( x[1] * x[4] + x[4] * (x[1] + x[2] + x[3]),
x[1] * x[4],
x[1] * x[4] + 1.0,
x[1] * (x[1] + x[2] + x[3])
)
)
)
}
eval_g_ineq <- function(x) {
constr <- c( 25 - x[1] * x[2] * x[3] * x[4] )
grad   <- c( -x[2]*x[3]*x[4],
-x[1]*x[3]*x[4],
-x[1]*x[2]*x[4],
-x[1]*x[2]*x[3]
)
return( list( "constraints"=constr, "jacobian"=grad ) )
}
eval_g_eq <- function(x) {
constr <- c( x[1]^2 + x[2]^2 + x[3]^2 + x[4]^2 - 40 )
grad <- c( 2.0*x[1],
2.0*x[2],
2.0*x[3],
2.0*x[4]
)
return( list( "constraints"=constr, "jacobian"=grad ) )
}
# Ahora definimos los valores iniciales
x0 <- c( 1, 5, 5, 1 )
# Y unos valores minimos y maximos que sirvan para acotar la solucion
lb <- c( 1, 1, 1, 1 )  # Valores minimos
ub <- c( 5, 5, 5, 5 )  # Valores maximos
# Especifiquemos el tipo de algoritmo y otras opciones
local_opts <- list(
"algorithm" = "NLOPT_LD_MMA",
"xtol_rel" = 1.0e-7
)
opts <- list(
"algorithm" = "NLOPT_LD_AUGLAG",
"xtol_rel" = 1.0e-7,
"maxeval" = 1000,
"local_opts" = local_opts
)
# Ahora si, invoquemos la funcion de optimizacion
res <- nloptr(
x0=x0,
eval_f=eval_f,
lb=lb,
ub=ub,
eval_g_ineq=eval_g_ineq,
eval_g_eq=eval_g_eq,
opts=opts
)
print(res)
library(nleqslv)
# El paquete nleqsl nos permite encontrar las raices de problemas de la
# forma f(x)=0, donde x es un vector en Rn
# Primer ejemplo: Una funcion muy facil: f(x)=x-4
muy_facil <- function(x){ return(x-4) } # Claramente, la solucion es x=4
x0 <- 2  # Debemos dar al algoritmo num?eico un valor inicial
resultados <- nleqslv(x0,muy_facil)  # Usamos nleqslv para encontrar x
x_opt <- resultados$x  # Sacamos el valor de x que resuelve la ecuacion
x_opt
# Segundo ejemplo: Misma funcion con argumento adicional
facil <- function(x,a,b){ return(x-a+b) } # Claramente, la solucion es x=a
# Note como el cuarto argumento es para la funcion "facil" (a=5)
resultados <- nleqslv(x0,facil,jac=NULL,b=0,a=5)
x_opt_2 <- resultados$x
x_opt_2
# Tercer ejemplo: Sistemas de ecuaciones
menos.facil <- function(X){
x <- X[1]
y <- X[2]
# La funcion recibe un vector y regresa un vector
return(c(x+y-2,x*y-1))
}
x0 <- c(1,1) # Note que esta vez el valor inicial es un vector con dos elementos
resultados <- nleqslv(x0,menos.facil)
x_opt_3 <- resultados$x
x_opt_3
install.packages('numDeriv')
library(numDeriv)
la_funcion <- function(x) return(x^2)
grad(la_funcion,2)             # Derivada de la funcion f(x)=x^2 evaluada en 2
grad(la_funcion,0)             # Derivada de la funcion f(x)=x^2 evaluada en 0
grad(la_funcion,-2)            # Derivada de la funcion f(x)=x^2 evaluada en -2
# Derivada de la funcion f(x)=x^2 evaluada en el intevalo discreto -2:0.1:2
grad(la_funcion,seq(-2,2,0.1))
# Ahora, intentemos con una funcion de R^2 a R^1
otra_funcion <- function(x) return(x[1]^2+x[2]^2)
grad(otra_funcion,c(2,2))   # Derivada de la funcion evaluada en (2,2)
grad(otra_funcion,c(1,2))   # Derivada de la funcion evaluada en (1,2)
grad(otra_funcion,c(0,0))   # Derivada de la funcion evaluada en (0,0)
grad(otra_funcion,c(-2,-2)) # Derivada de la funcion evaluada en (-2,-2)
funcion_1_a_2 <- function(x) return(c(x^2,x^3))
jacobian(funcion_1_a_2,1) # Jacobiano evaluado en 1
jacobian(funcion_1_a_2,2) # Jacobiiano evaluado en 2
# Jacobiano de funcion de R^2 a R^2
funcion_2_a_2 <- function(x) return(c(x[1]^2+x[2]^2,x[1]^3+x[2]^3))
jacobian(funcion_2_a_2, c(1,1)) # Jacobiano evaluado en (1,1)
jacobian(funcion_2_a_2, c(1,1)) # Jacobiano evaluado en (2,2)
## Hessiana ##
# Finalmente, el comando hessian() nos permite calcular la matriz Hessiana de
# una funcion de R^n a R^1
la_ultima_funcion <- function(x) return(x[1]^2+x[2]^3)
hessian(la_ultima_funcion,c(1,1)) # Hessiana evaluada en el punto (1,1)
hessian(la_ultima_funcion,c(1,2)) # Hessiana evaluada en el punto (1,2)
hessian(la_ultima_funcion,c(2,1)) # Hessiana evaluada en el punto (2,1)
hessian(la_ultima_funcion,c(0,0)) # Hessiana evaluada en el punto (0,0)
# La instalacion base de R cuenta con una funcion para realizar integracion
# numerica que ya vimos anteriormente
una_funcion <- function(x) return((x^2)*cos(x)) # Definimos una funcion
integrate(una_funcion,-1,5) # Integramos, asi de sencillo
# Tambien podemos utilizarla para integrar sobre todo el dominio de la funcion
# (siempre que sea convergente)
otra_funcion <- function(x) return(1/((x+1)*sqrt(x)))
integrate(otra_funcion, lower = 0, upper = Inf)
library(R2Cuba)
# Definamos una funcion de R^2 a R^1
la_integral <- function(x) return(x[1]^2+x[2]^2)
num.x <-2  # Numero de inputs
num.y <- 1 # Numero de outputs
# Integracion usando algoritmo deterministico
cuhre(num.x, num.y, la_integral,lower=c(-1,-1),upper=c(1,1))
# Integracion por Monte-Carlo
vegas(num.x, num.y, la_integral,lower=c(-1,-1),upper=c(1,1))
# Definamos una funcion de R^2 a R^1
la_integral_2 <- function(x) return(c(x[1]^2+x[2]^2,x[1]^3+x[2]^3))
num.x <-2  # Numero de inputs
num.y <- 2 # Numero de outputs
# Integracion usando algoritmo deterministico
cuhre(num.x, num.y, la_integral_2,lower=c(0,0),upper=c(1,1))
# Integracion por Monte-Carlo
vegas(num.x, num.y, la_integral_2,lower=c(0,0),upper=c(1,1))
# Definamos una funcion de R^2 a R^1
la_integral <- function(x) return(x[1]^2+x[2]^2)
num.x <-2  # Numero de inputs
num.y <- 1 # Numero de outputs
# Integracion usando algoritmo deterministico
cuhre(num.x, num.y, la_integral,lower=c(-1,-1),upper=c(1,1))
vegas(num.x, num.y, la_integral,lower=c(-1,-1),upper=c(1,1))
la_integral_2 <- function(x) return(c(x[1]^2+x[2]^2,x[1]^3+x[2]^3))
num.x <-2  # Numero de inputs
num.y <- 2 # Numero de outputs
la_integral_2 <- function(x) return(c(x[1]^2+x[2]^2,x[1]^3+x[2]^3))
num.x <- 2 # Numero de inputs
num.y <- 2 # Numero de outputs
# Integracion usando algoritmo deterministico
cuhre(num.x, num.y, la_integral_2,lower=c(0,0),upper=c(1,1))
vegas(num.x, num.y, la_integral,lower=c(-1,-1),upper=c(1,1))
la_integral <- function(x) return(x[1]^2+x[2]^2)
num.x <- 2 # Numero de inputs
num.y <- 1 # Numero de outputs
# Integracion usando algoritmo deterministico
cuhre(num.x, num.y, la_integral,lower=c(-1,-1),upper=c(1,1))
la_integral_2 <- function(x) return(c(x[1]^2+x[2]^2,x[1]^3+x[2]^3))
num.x <- 2 # Numero de inputs
num.y <- 2 # Numero de outputs
# Integracion usando algoritmo deterministico
cuhre(num.x, num.y, la_integral_2,lower=c(0,0),upper=c(1,1))
cuhre(num.x, num.y, la_integral_2,lower=c(-1,-1),upper=c(1,1))
la_integral_2 <- function(x) return(c(x[1]^2+x[2]^2,x[1]^3+x[2]^3))
num.x <- 2 # Numero de inputs
num.y <- 2 # Numero de outputs
# Integracion usando algoritmo deterministico
cuhre(num.x, num.y, la_integral_2,lower=c(-1,-1),upper=c(1,1))
vegas(num.x, num.y, la_integral_2,lower=c(-1,-1),upper=c(1,1))
# Definamos una funcion de R^2 a R^1
la_integral <- function(x) return(x[1]^2+x[2]^2)
num.x <- 2 # Numero de inputs
num.y <- 1 # Numero de outputs
# Integracion usando algoritmo deterministico
cuhre(num.x, num.y, la_integral,lower=c(0,0),upper=c(1,1))
# Integracion por Monte-Carlo
vegas(num.x, num.y, la_integral,lower=c(0,0),upper=c(1,1))
la_integral_2 <- function(x) return(c(x[1]^2+x[2]^2,x[1]^3+x[2]^3))
num.x <- 2 # Numero de inputs
num.y <- 2 # Numero de outputs
# Integracion usando algoritmo deterministico
cuhre(num.x, num.y, la_integral_2,lower=c(0,0),upper=c(1,1))
# Ejemplo: Definir la funcion x^2
sym_fun <- expression(x^2)
sym_fun
sym_fun[1]
sym_fun[[1]]
eval(sym_fun, envir=list(x=2))
x=2
eval(sym_fun)
all.vars(sym_fun)
D_sym_fun <- D(sym_fun,"x")
D_sym_fun
eval(D_sym_fun)
sym_more_fun <- expression(
x^2 + y^2,
x*y
)
sym_more_fun
sym_more_fun[1]
sym_more_fun[[2]]
all.vars(sym_more_fun)
D_sym_fun_2 <- deriv(sym_more_fun,c("x","y"))
D_sym_fun_2
x=1; y=1;
eval(D_sym_fun_2)
?deriv
D_sym_fun_2
eval(D_sym_fun_2)
D_sym_fun_2
D_sym_fun_[1]
D_sym_fun[1]
D_sym_fun[[1]]
D_sym_fun[1]
# Primero, definamos una funcion de varias variables
sym_more_fun <- expression(
x^2 + y^2,
x*y
)
sym_more_fun
sym_more_fun[1]
sym_more_fun[[2]]
all.vars(sym_more_fun)
# Ahora, obtengamos el gradiente usando deriv()
D_sym_fun_2 <- deriv(sym_more_fun,c("x","y"))
D_sym_fun_2
sym_more_fun <- expression(
x^2 + y^2,
x*y
)
sym_more_fun
sym_more_fun[1]
sym_more_fun[[2]]
all.vars(sym_more_fun)
D_sym_fun_2 <- deriv(sym_more_fun,c("x","y"))
D_sym_fun_2
# Primero, definamos una funcion de varias variables
sym_more_fun <- expression(
x^2 + y^2,
x^3 + y^3
)
sym_more_fun
sym_more_fun[1]
sym_more_fun[[2]]
all.vars(sym_more_fun)
# Ahora, obtengamos el gradiente usando deriv()
D_sym_fun_2 <- deriv(sym_more_fun,c("x","y"))
D_sym_fun_2
# Primero, definamos una funcion de varias variables
sym_more_fun <- expression(
x^2 + y^2
)
sym_more_fun
sym_more_fun[1]
sym_more_fun[[1]]
all.vars(sym_more_fun)
# Ahora, obtengamos el gradiente usando deriv()
D_sym_fun_2 <- deriv(sym_more_fun,c("x","y"))
D_sym_fun_2
# Podemos evaluar esta expresion en un punto determinado
x=1; y=1;
eval(D_sym_fun_2)
# Tambien podemos usar derive para encontrar la matriz Hessiana de f(x,y)
hessian_mat <- deriv(sym_more_fun,c("x","y"),hessian=T)
hessian_mat
# Primero, definamos una funcion de varias variables
sym_more_fun <- expression(
x^3 + y^3
)
sym_more_fun
sym_more_fun[1]
sym_more_fun[[1]]
all.vars(sym_more_fun)
# Ahora, obtengamos el gradiente usando deriv()
D_sym_fun_2 <- deriv(sym_more_fun,c("x","y"))
D_sym_fun_2
# Podemos evaluar esta expresion en un punto determinado
x=1; y=1;
eval(D_sym_fun_2)
# Tambien podemos usar derive para encontrar la matriz Hessiana de f(x,y)
hessian_mat <- deriv(sym_more_fun,c("x","y"),hessian=T)
hessian_mat
hessian_mat <- deriv(sym_more_fun,c("x","y"),hessian=T)
hessian_mat
# Ahora necesitamos attr() para recuperar la matriz Hessiana
hessian_fun <- attr(eval(hessian_mat),"hessian")
hessian_fun[1,,]
# Tambien podemos usar attr() para recuperar el gradiente
gradient_fun <- attr(eval(hessian_mat),"gradient")
gradient_fun
gradient_fun
sym_jacobian <- sapply(sym_more_fun, deriv,c("x","y"))
eval(sym_jacobian[1])
eval(sym_jacobian[2])
sym_more_fun_2 <- expression(
x^3 + y^3,
x^2 + y^2
)
sym_jacobian <- sapply(sym_more_fun_2, deriv,c("x","y"))
eval(sym_jacobian[1])
eval(sym_jacobian[2])
sym_jacobian
# Estas funciones permiten construir "input" para algoritmos numericos
obj_fun <- expression(x^3+x^2+x+1)
foc <- D(obj_fun,"x")
foc
new_fun <-function(x){return(eval(foc))}
new_fun(1)
windows(); plot(-10:10,new_fun(-10:10))
resultados <- nleqslv(1,new_fun)  # Usamos nleqslv para encontrar x
x_opt <- resultados$x  # Sacamos el valor de x que resuelve la ecuacion
x_opt
